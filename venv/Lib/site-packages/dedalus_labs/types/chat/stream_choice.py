# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.

from typing import Optional
from typing_extensions import Literal

from ..._models import BaseModel
from .choice_delta import ChoiceDelta
from .stream_choice_logprobs import StreamChoiceLogprobs

__all__ = ["StreamChoice"]


class StreamChoice(BaseModel):
    """Schema for ChatCompletionStreamResponseChoicesItem.

    Fields:
    - delta (required): ChatCompletionStreamResponseDelta
    - logprobs (optional): ChatCompletionStreamResponseChoicesItemLogprobs
    - finish_reason (required): Literal["stop", "length", "tool_calls", "content_filter", "function_call"]
    - index (required): int
    """

    delta: ChoiceDelta
    """A chat completion delta generated by streamed model responses."""

    finish_reason: Optional[Literal["stop", "length", "tool_calls", "content_filter", "function_call"]] = None
    """The reason the model stopped generating tokens.

    This will be `stop` if the model hit a natural stop point or a provided stop
    sequence, `length` if the maximum number of tokens specified in the request was
    reached, `content_filter` if content was omitted due to a flag from our content
    filters, `tool_calls` if the model called a tool, or `function_call`
    (deprecated) if the model called a function. Will be `null` until the stream is
    complete, then one of the above values.
    """

    index: int
    """The index of the choice in the list of choices."""

    logprobs: Optional[StreamChoiceLogprobs] = None
    """Log probability information for the choice.

    Fields:

    - content (required): list[ChatCompletionTokenLogprob]
    - refusal (required): list[ChatCompletionTokenLogprob]
    """
