# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.

from typing import Optional
from typing_extensions import Literal

from ..._models import BaseModel
from .choice_logprobs import ChoiceLogprobs
from .chat_completion_message import ChatCompletionMessage

__all__ = ["Choice"]


class Choice(BaseModel):
    """A chat completion choice.

    OpenAI-compatible choice object for non-streaming responses.
    Part of the ChatCompletion response.
    """

    index: int
    """The index of the choice in the list of choices."""

    message: ChatCompletionMessage
    """A chat completion message generated by the model."""

    finish_reason: Optional[Literal["stop", "length", "tool_calls", "content_filter", "function_call"]] = None
    """The reason the model stopped generating tokens.

    This will be `stop` if the model hit a natural stop point or a provided stop
    sequence, `length` if the maximum number of tokens specified in the request was
    reached, `content_filter` if content was omitted due to a flag from our content
    filters, `tool_calls` if the model called a tool, or `function_call`
    (deprecated) if the model called a function.
    """

    logprobs: Optional[ChoiceLogprobs] = None
    """Log probability information for the choice."""
